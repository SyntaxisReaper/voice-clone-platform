#!/usr/bin/env python3
"""
Voice Model Validation and Testing for VCaaS Platform
Tests trained models for quality, integration, and API compatibility
"""

import os
import sys
import json
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple
import random
import time

# Add backend to path for imports
sys.path.append(str(Path(__file__).parent.parent))

class VoiceModelValidator:
    """Validates trained voice models and tests integration"""
    
    def __init__(self, registry_path: str):
        self.registry_path = Path(registry_path)
        self.voice_registry = None
        self.validation_results = {}
        
    def load_voice_registry(self) -> bool:
        """Load the voice registry"""
        if not self.registry_path.exists():
            print(f"‚ùå Voice registry not found: {self.registry_path}")
            return False
        
        try:
            with open(self.registry_path, 'r') as f:
                self.voice_registry = json.load(f)
            
            print(f"‚úÖ Loaded voice registry with {len(self.voice_registry['voices'])} voices")
            return True
            
        except Exception as e:
            print(f"‚ùå Failed to load voice registry: {e}")
            return False
    
    def validate_model_files(self, voice_id: str, voice_profile: Dict) -> Dict:
        """Validate that model files exist and are accessible"""
        print(f"  üîç Validating model files...")
        
        model_info = voice_profile['model_info']
        validation = {
            'model_files': {},
            'file_sizes': {},
            'accessibility': True,
            'total_size_mb': 0
        }
        
        required_files = {
            'model_path': 'PyTorch Model',
            'config_path': 'Model Configuration',
            'vocoder_config': 'Vocoder Configuration'
        }\n        \n        for file_key, file_desc in required_files.items():\n            file_path = Path(model_info[file_key])\n            \n            validation['model_files'][file_key] = {\n                'path': str(file_path),\n                'exists': file_path.exists(),\n                'description': file_desc\n            }\n            \n            if file_path.exists():\n                file_size = file_path.stat().st_size\n                validation['file_sizes'][file_key] = file_size\n                validation['total_size_mb'] += file_size / (1024 * 1024)\n            else:\n                validation['accessibility'] = False\n                print(f\"    ‚ö†Ô∏è  Missing: {file_desc} at {file_path}\")\n        \n        if validation['accessibility']:\n            print(f\"    ‚úÖ All model files present ({validation['total_size_mb']:.2f} MB total)\")\n        \n        return validation\n    \n    def test_model_configuration(self, voice_id: str, voice_profile: Dict) -> Dict:\n        \"\"\"Test model configuration validity\"\"\"\n        print(f\"  ‚öôÔ∏è  Testing model configuration...\")\n        \n        model_info = voice_profile['model_info']\n        config_test = {\n            'architecture_valid': True,\n            'training_config_valid': True,\n            'compatibility_score': 0.0,\n            'issues': []\n        }\n        \n        # Test architecture configuration\n        architecture = model_info['architecture']\n        required_arch_params = ['encoder_dim', 'decoder_dim', 'attention_heads', 'mel_channels']\n        \n        for param in required_arch_params:\n            if param not in architecture:\n                config_test['architecture_valid'] = False\n                config_test['issues'].append(f\"Missing architecture parameter: {param}\")\n            elif not isinstance(architecture[param], int) or architecture[param] <= 0:\n                config_test['architecture_valid'] = False\n                config_test['issues'].append(f\"Invalid architecture parameter: {param}\")\n        \n        # Test training configuration\n        training_config = model_info['training_config']\n        required_training_params = ['epochs', 'batch_size', 'learning_rate']\n        \n        for param in required_training_params:\n            if param not in training_config:\n                config_test['training_config_valid'] = False\n                config_test['issues'].append(f\"Missing training parameter: {param}\")\n        \n        # Calculate compatibility score\n        if config_test['architecture_valid'] and config_test['training_config_valid']:\n            config_test['compatibility_score'] = 1.0\n        elif config_test['architecture_valid'] or config_test['training_config_valid']:\n            config_test['compatibility_score'] = 0.5\n        \n        if config_test['issues']:\n            for issue in config_test['issues']:\n                print(f\"    ‚ö†Ô∏è  {issue}\")\n        else:\n            print(f\"    ‚úÖ Configuration valid (compatibility: {config_test['compatibility_score']:.1f})\")\n        \n        return config_test\n    \n    def simulate_tts_inference(self, voice_id: str, voice_profile: Dict) -> Dict:\n        \"\"\"Simulate TTS inference testing\"\"\"\n        print(f\"  üé§ Simulating TTS inference...\")\n        \n        test_texts = [\n            \"Hello, this is a test of the text-to-speech synthesis system.\",\n            \"The quick brown fox jumps over the lazy dog.\",\n            \"Voice cloning technology enables realistic speech synthesis.\",\n            \"Testing various phonemes and prosodic patterns in speech.\"\n        ]\n        \n        inference_test = {\n            'test_cases': [],\n            'average_quality': 0.0,\n            'average_latency': 0.0,\n            'success_rate': 0.0,\n            'audio_quality_metrics': {}\n        }\n        \n        quality_metrics = voice_profile['quality_metrics']\n        base_quality = quality_metrics['overall_score']\n        \n        successful_tests = 0\n        total_latency = 0\n        total_quality = 0\n        \n        for i, text in enumerate(test_texts):\n            # Simulate inference\n            latency = random.uniform(0.5, 2.0) * len(text) / 50  # Simulate based on text length\n            \n            # Simulate quality based on model quality and text complexity\n            text_complexity = len(set(text.lower())) / len(text)  # Character diversity\n            quality_variance = random.uniform(-0.05, 0.05)\n            estimated_quality = min(1.0, base_quality - (text_complexity * 0.1) + quality_variance)\n            \n            success = estimated_quality > 0.6 and latency < 10.0\n            \n            test_case = {\n                'test_id': i + 1,\n                'text': text,\n                'text_length': len(text),\n                'estimated_latency': latency,\n                'estimated_quality': estimated_quality,\n                'success': success\n            }\n            \n            inference_test['test_cases'].append(test_case)\n            \n            if success:\n                successful_tests += 1\n                total_latency += latency\n                total_quality += estimated_quality\n        \n        # Calculate averages\n        if successful_tests > 0:\n            inference_test['average_latency'] = total_latency / successful_tests\n            inference_test['average_quality'] = total_quality / successful_tests\n        \n        inference_test['success_rate'] = successful_tests / len(test_texts)\n        \n        # Audio quality metrics simulation\n        inference_test['audio_quality_metrics'] = {\n            'mel_spectrogram_similarity': random.uniform(0.7, 0.95),\n            'speaker_similarity': base_quality * random.uniform(0.9, 1.1),\n            'prosody_naturalness': random.uniform(0.6, 0.9),\n            'intelligibility': random.uniform(0.8, 0.98)\n        }\n        \n        print(f\"    ‚úÖ Inference test: {successful_tests}/{len(test_texts)} successful\")\n        print(f\"    ‚è±Ô∏è  Average latency: {inference_test['average_latency']:.2f}s\")\n        print(f\"    ‚≠ê Average quality: {inference_test['average_quality']:.3f}\")\n        \n        return inference_test\n    \n    def test_api_compatibility(self, voice_id: str, voice_profile: Dict) -> Dict:\n        \"\"\"Test API compatibility and response format\"\"\"\n        print(f\"  üåê Testing API compatibility...\")\n        \n        api_test = {\n            'response_format_valid': True,\n            'required_fields_present': True,\n            'data_types_correct': True,\n            'api_ready': True,\n            'missing_fields': [],\n            'invalid_types': []\n        }\n        \n        # Required API fields\n        required_fields = {\n            'voice_id': str,\n            'display_name': str,\n            'description': str,\n            'voice_characteristics': dict,\n            'quality_metrics': dict,\n            'usage_settings': dict\n        }\n        \n        for field, expected_type in required_fields.items():\n            if field not in voice_profile:\n                api_test['required_fields_present'] = False\n                api_test['missing_fields'].append(field)\n            elif not isinstance(voice_profile[field], expected_type):\n                api_test['data_types_correct'] = False\n                api_test['invalid_types'].append(f\"{field}: expected {expected_type.__name__}\")\n        \n        # Test nested required fields\n        if 'voice_characteristics' in voice_profile:\n            voice_chars = voice_profile['voice_characteristics']\n            required_char_fields = ['language', 'gender', 'accent']\n            \n            for field in required_char_fields:\n                if field not in voice_chars:\n                    api_test['missing_fields'].append(f\"voice_characteristics.{field}\")\n        \n        api_test['api_ready'] = (api_test['response_format_valid'] and \n                                api_test['required_fields_present'] and \n                                api_test['data_types_correct'])\n        \n        if api_test['api_ready']:\n            print(f\"    ‚úÖ API compatible\")\n        else:\n            for field in api_test['missing_fields']:\n                print(f\"    ‚ö†Ô∏è  Missing field: {field}\")\n            for invalid in api_test['invalid_types']:\n                print(f\"    ‚ö†Ô∏è  Invalid type: {invalid}\")\n        \n        return api_test\n    \n    def calculate_overall_score(self, validation_results: Dict) -> float:\n        \"\"\"Calculate overall validation score\"\"\"\n        weights = {\n            'file_validation': 0.2,\n            'config_test': 0.2,\n            'inference_test': 0.4,\n            'api_test': 0.2\n        }\n        \n        scores = {\n            'file_validation': 1.0 if validation_results['file_validation']['accessibility'] else 0.0,\n            'config_test': validation_results['config_test']['compatibility_score'],\n            'inference_test': validation_results['inference_test']['success_rate'],\n            'api_test': 1.0 if validation_results['api_test']['api_ready'] else 0.5\n        }\n        \n        overall_score = sum(scores[key] * weights[key] for key in weights.keys())\n        return overall_score\n    \n    def validate_voice_model(self, voice_id: str) -> Dict:\n        \"\"\"Validate a single voice model\"\"\"\n        print(f\"\\nüé§ Validating Voice Model: {voice_id}\")\n        print(\"-\" * 60)\n        \n        voice_profile = self.voice_registry['voices'][voice_id]\n        \n        validation_start = time.time()\n        \n        # Run validation tests\n        validation_results = {\n            'voice_id': voice_id,\n            'validation_start': datetime.now().isoformat(),\n            'file_validation': self.validate_model_files(voice_id, voice_profile),\n            'config_test': self.test_model_configuration(voice_id, voice_profile),\n            'inference_test': self.simulate_tts_inference(voice_id, voice_profile),\n            'api_test': self.test_api_compatibility(voice_id, voice_profile)\n        }\n        \n        # Calculate overall score\n        overall_score = self.calculate_overall_score(validation_results)\n        validation_results['overall_score'] = overall_score\n        validation_results['validation_duration'] = time.time() - validation_start\n        \n        # Determine status\n        if overall_score >= 0.9:\n            status = \"‚úÖ EXCELLENT\"\n        elif overall_score >= 0.8:\n            status = \"‚úÖ GOOD\"\n        elif overall_score >= 0.7:\n            status = \"‚ö†Ô∏è  ACCEPTABLE\"\n        else:\n            status = \"‚ùå NEEDS WORK\"\n        \n        validation_results['status'] = status\n        \n        print(f\"\\n  üìä Overall Score: {overall_score:.3f} - {status}\")\n        print(f\"  ‚è±Ô∏è  Validation Time: {validation_results['validation_duration']:.2f}s\")\n        \n        return validation_results\n    \n    def validate_all_models(self) -> Dict:\n        \"\"\"Validate all models in the registry\"\"\"\n        if not self.voice_registry:\n            return {}\n        \n        print(\"üöÄ Voice Model Validation Suite\")\n        print(\"=\" * 70)\n        print(f\"üìä Total Models to Validate: {len(self.voice_registry['voices'])}\")\n        print(\"=\" * 70)\n        \n        validation_session = {\n            'session_start': datetime.now().isoformat(),\n            'total_models': len(self.voice_registry['voices']),\n            'results': {},\n            'summary': {\n                'excellent': 0,\n                'good': 0,\n                'acceptable': 0,\n                'needs_work': 0,\n                'average_score': 0.0\n            }\n        }\n        \n        total_score = 0\n        \n        for voice_id in self.voice_registry['voices'].keys():\n            try:\n                result = self.validate_voice_model(voice_id)\n                validation_session['results'][voice_id] = result\n                \n                # Update summary\n                score = result['overall_score']\n                total_score += score\n                \n                if score >= 0.9:\n                    validation_session['summary']['excellent'] += 1\n                elif score >= 0.8:\n                    validation_session['summary']['good'] += 1\n                elif score >= 0.7:\n                    validation_session['summary']['acceptable'] += 1\n                else:\n                    validation_session['summary']['needs_work'] += 1\n                \n            except Exception as e:\n                print(f\"  ‚ùå Validation failed for {voice_id}: {e}\")\n                continue\n        \n        # Calculate averages\n        successful_validations = len(validation_session['results'])\n        if successful_validations > 0:\n            validation_session['summary']['average_score'] = total_score / successful_validations\n        \n        validation_session['session_end'] = datetime.now().isoformat()\n        \n        return validation_session\n\ndef main():\n    parser = argparse.ArgumentParser(description='Validate voice models for VCaaS platform')\n    parser.add_argument('--registry', default='../models/vcaas_voice_registry/voice_registry.json',\n                       help='Path to voice registry file')\n    parser.add_argument('--output', help='Output file for validation results')\n    parser.add_argument('--voice_id', help='Validate specific voice ID only')\n    \n    args = parser.parse_args()\n    \n    validator = VoiceModelValidator(args.registry)\n    \n    if not validator.load_voice_registry():\n        return 1\n    \n    # Run validation\n    if args.voice_id:\n        if args.voice_id in validator.voice_registry['voices']:\n            result = validator.validate_voice_model(args.voice_id)\n            validation_session = {\n                'single_validation': True,\n                'voice_id': args.voice_id,\n                'result': result\n            }\n        else:\n            print(f\"‚ùå Voice ID not found: {args.voice_id}\")\n            return 1\n    else:\n        validation_session = validator.validate_all_models()\n    \n    # Save results\n    if args.output:\n        output_path = Path(args.output)\n    else:\n        registry_dir = Path(args.registry).parent\n        output_path = registry_dir / f\"validation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n    \n    with open(output_path, 'w') as f:\n        json.dump(validation_session, f, indent=2)\n    \n    # Print summary\n    if 'summary' in validation_session:\n        summary = validation_session['summary']\n        print(\"\\n\" + \"=\" * 70)\n        print(\"üéâ Validation Complete!\")\n        print(\"=\" * 70)\n        print(f\"üìà Total Models: {validation_session['total_models']}\")\n        print(f\"‚úÖ Excellent (‚â•0.9): {summary['excellent']}\")\n        print(f\"‚úÖ Good (‚â•0.8): {summary['good']}\")\n        print(f\"‚ö†Ô∏è  Acceptable (‚â•0.7): {summary['acceptable']}\")\n        print(f\"‚ùå Needs Work (<0.7): {summary['needs_work']}\")\n        print(f\"üìä Average Score: {summary['average_score']:.3f}\")\n        print(f\"üíæ Results saved: {output_path}\")\n    \n    return 0\n\nif __name__ == \"__main__\":\n    exit(main())